{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b71c6b",
   "metadata": {},
   "source": [
    "# IND320 — Project Part 4  \n",
    "### Elhub Data Processing, Analysis, and Streamlit Application\n",
    "\n",
    "This notebook contains the main steps of my work in Part 4 of the project.  \n",
    "It documents how I collected hourly production and consumption data from the Elhub API, cleaned and combined the datasets for 2021–2024, and stored them in both MongoDB and Cassandra.  \n",
    "The notebook also includes checks of the loaded data, basic summaries, screenshots from the Streamlit app, and a short log describing the development process and challenges I worked through.  \n",
    "All code cells are commented so the workflow can be followed and reproduced easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b67e4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv pymongo dnspython pyspark plotly requests\n",
    "# Spark Cassandra connector is provided via spark.jars.packages in the SparkSession builder.\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple\n",
    "import time, os, json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from zoneinfo import ZoneInfo\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89f1262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11528c39",
   "metadata": {},
   "source": [
    "## 1) Config (Elhub + global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "339073b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elhub API\n",
    "ELHUB_BASE_URL = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "DATASET_PROD = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "DATASET_CONS = \"CONSUMPTION_PER_GROUP_MBA_HOUR\"\n",
    "\n",
    "UTC = ZoneInfo(\"UTC\")\n",
    "PAUSE_S = 0.4\n",
    "TIMEOUT_S = 30\n",
    "\n",
    "# Ranges\n",
    "PROD_YEARS = [2022, 2023, 2024]        # append after your 2021 data (already done in Part 2)\n",
    "CONS_YEARS = [2021, 2022, 2023, 2024]  # new dataset\n",
    "\n",
    "# Utility: build monthly (start, end) UTC windows for a given year\n",
    "def monthly_windows(year: int) -> List[Tuple[datetime, datetime]]:\n",
    "    wins = []\n",
    "    for m in range(1, 13):\n",
    "        start = datetime(year, m, 1, 0, 0, 0, tzinfo=UTC)\n",
    "        next_year = year + (m // 12)\n",
    "        next_month = (m % 12) + 1\n",
    "        end = datetime(next_year, next_month, 1, 0, 0, 0, tzinfo=UTC)\n",
    "        wins.append((start, end))\n",
    "    return wins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec54d4",
   "metadata": {},
   "source": [
    "## 2) Fetch helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd6fc2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "\n",
    "def fetch_elhub(dataset: str, start_dt: datetime, end_dt: datetime) -> List[Dict]:\n",
    "    \"\"\"Fetch one monthly window from Elhub and return flattened hourly records list.\"\"\"\n",
    "    params = {\n",
    "        \"dataset\": dataset,\n",
    "        \"startDate\": start_dt.isoformat(),  # UTC (no DST issues)\n",
    "        \"endDate\": end_dt.isoformat(),\n",
    "    }\n",
    "    try:\n",
    "        r = session.get(ELHUB_BASE_URL, params=params, timeout=TIMEOUT_S)\n",
    "        r.raise_for_status()\n",
    "        payload = r.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"{start_dt:%Y-%m} — omitted (I/O): {e}\")\n",
    "        return []\n",
    "\n",
    "    out = []\n",
    "    for area in payload.get(\"data\", []):\n",
    "        attrs = area.get(\"attributes\") or {}\n",
    "        # key is productionPerGroupMbaHour OR consumptionPerGroupMbaHour depending on dataset\n",
    "        key = \"productionPerGroupMbaHour\" if \"PRODUCTION\" in dataset else \"consumptionPerGroupMbaHour\"\n",
    "        chunk = attrs.get(key) or []\n",
    "        if isinstance(chunk, list):\n",
    "            out.extend(chunk)\n",
    "    return out\n",
    "\n",
    "def fetch_years(dataset: str, years: List[int]) -> pd.DataFrame:\n",
    "    all_records = []\n",
    "    print(f\"Fetching {dataset} for years: {years}\")\n",
    "    for y in years:\n",
    "        for (start_dt, end_dt) in monthly_windows(y):\n",
    "            rows = fetch_elhub(dataset, start_dt, end_dt)\n",
    "            all_records.extend(rows)\n",
    "            time.sleep(PAUSE_S)\n",
    "    if not all_records:\n",
    "        print(\"No data retrieved.\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(all_records)\n",
    "    # Normalize times to UTC (keep tz-aware)\n",
    "    for col in (\"startTime\", \"endTime\", \"lastUpdatedTime\"):\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], utc=True, errors=\"coerce\")\n",
    "    # Order columns\n",
    "    preferred = [\"startTime\", \"endTime\", \"lastUpdatedTime\", \"priceArea\", \"productionGroup\", \"quantityKwh\"]\n",
    "    ordered = [c for c in preferred if c in df.columns] + [c for c in df.columns if c not in preferred]\n",
    "    df = df[ordered]\n",
    "    print(f\"Retrieved rows: {len(df):,}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f375224",
   "metadata": {},
   "source": [
    " ## 3) Fetch production (2022–2024) and consumption (2021–2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26df9856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching PRODUCTION_PER_GROUP_MBA_HOUR for years: [2022, 2023, 2024]\n",
      "Retrieved rows: 657,600\n",
      "Fetching CONSUMPTION_PER_GROUP_MBA_HOUR for years: [2021, 2022, 2023, 2024]\n",
      "Retrieved rows: 876,600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>startTime</th>\n",
       "      <th>endTime</th>\n",
       "      <th>lastUpdatedTime</th>\n",
       "      <th>priceArea</th>\n",
       "      <th>productionGroup</th>\n",
       "      <th>quantityKwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-31 23:00:00+00:00</td>\n",
       "      <td>2022-01-01 00:00:00+00:00</td>\n",
       "      <td>2025-02-01 17:02:57+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1291422.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01 00:00:00+00:00</td>\n",
       "      <td>2022-01-01 01:00:00+00:00</td>\n",
       "      <td>2025-02-01 17:02:57+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1246209.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01 01:00:00+00:00</td>\n",
       "      <td>2022-01-01 02:00:00+00:00</td>\n",
       "      <td>2025-02-01 17:02:57+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1271757.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01 02:00:00+00:00</td>\n",
       "      <td>2022-01-01 03:00:00+00:00</td>\n",
       "      <td>2025-02-01 17:02:57+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1204251.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01 03:00:00+00:00</td>\n",
       "      <td>2022-01-01 04:00:00+00:00</td>\n",
       "      <td>2025-02-01 17:02:57+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1202086.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  startTime                   endTime  \\\n",
       "0 2021-12-31 23:00:00+00:00 2022-01-01 00:00:00+00:00   \n",
       "1 2022-01-01 00:00:00+00:00 2022-01-01 01:00:00+00:00   \n",
       "2 2022-01-01 01:00:00+00:00 2022-01-01 02:00:00+00:00   \n",
       "3 2022-01-01 02:00:00+00:00 2022-01-01 03:00:00+00:00   \n",
       "4 2022-01-01 03:00:00+00:00 2022-01-01 04:00:00+00:00   \n",
       "\n",
       "            lastUpdatedTime priceArea productionGroup  quantityKwh  \n",
       "0 2025-02-01 17:02:57+00:00       NO1           hydro    1291422.4  \n",
       "1 2025-02-01 17:02:57+00:00       NO1           hydro    1246209.4  \n",
       "2 2025-02-01 17:02:57+00:00       NO1           hydro    1271757.0  \n",
       "3 2025-02-01 17:02:57+00:00       NO1           hydro    1204251.8  \n",
       "4 2025-02-01 17:02:57+00:00       NO1           hydro    1202086.9  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(657600, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>startTime</th>\n",
       "      <th>endTime</th>\n",
       "      <th>lastUpdatedTime</th>\n",
       "      <th>priceArea</th>\n",
       "      <th>quantityKwh</th>\n",
       "      <th>consumptionGroup</th>\n",
       "      <th>meteringPointCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-12-31 23:00:00+00:00</td>\n",
       "      <td>2021-01-01 00:00:00+00:00</td>\n",
       "      <td>2024-12-20 09:35:40+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>177071.56</td>\n",
       "      <td>cabin</td>\n",
       "      <td>100607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-01 00:00:00+00:00</td>\n",
       "      <td>2021-01-01 01:00:00+00:00</td>\n",
       "      <td>2024-12-20 09:35:40+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>171335.12</td>\n",
       "      <td>cabin</td>\n",
       "      <td>100607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-01 01:00:00+00:00</td>\n",
       "      <td>2021-01-01 02:00:00+00:00</td>\n",
       "      <td>2024-12-20 09:35:40+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>164912.02</td>\n",
       "      <td>cabin</td>\n",
       "      <td>100607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-01 02:00:00+00:00</td>\n",
       "      <td>2021-01-01 03:00:00+00:00</td>\n",
       "      <td>2024-12-20 09:35:40+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>160265.77</td>\n",
       "      <td>cabin</td>\n",
       "      <td>100607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-01 03:00:00+00:00</td>\n",
       "      <td>2021-01-01 04:00:00+00:00</td>\n",
       "      <td>2024-12-20 09:35:40+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>159828.69</td>\n",
       "      <td>cabin</td>\n",
       "      <td>100607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  startTime                   endTime  \\\n",
       "0 2020-12-31 23:00:00+00:00 2021-01-01 00:00:00+00:00   \n",
       "1 2021-01-01 00:00:00+00:00 2021-01-01 01:00:00+00:00   \n",
       "2 2021-01-01 01:00:00+00:00 2021-01-01 02:00:00+00:00   \n",
       "3 2021-01-01 02:00:00+00:00 2021-01-01 03:00:00+00:00   \n",
       "4 2021-01-01 03:00:00+00:00 2021-01-01 04:00:00+00:00   \n",
       "\n",
       "            lastUpdatedTime priceArea  quantityKwh consumptionGroup  \\\n",
       "0 2024-12-20 09:35:40+00:00       NO1    177071.56            cabin   \n",
       "1 2024-12-20 09:35:40+00:00       NO1    171335.12            cabin   \n",
       "2 2024-12-20 09:35:40+00:00       NO1    164912.02            cabin   \n",
       "3 2024-12-20 09:35:40+00:00       NO1    160265.77            cabin   \n",
       "4 2024-12-20 09:35:40+00:00       NO1    159828.69            cabin   \n",
       "\n",
       "   meteringPointCount  \n",
       "0              100607  \n",
       "1              100607  \n",
       "2              100607  \n",
       "3              100607  \n",
       "4              100607  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(876600, 7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_prod_22_24 = fetch_years(DATASET_PROD, PROD_YEARS)\n",
    "df_cons_21_24 = fetch_years(DATASET_CONS, CONS_YEARS)\n",
    "\n",
    "display(df_prod_22_24.head())\n",
    "display(df_prod_22_24.shape)\n",
    "display(df_cons_21_24.head())\n",
    "display(df_cons_21_24.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6309c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b990426d",
   "metadata": {},
   "source": [
    " ##  4) Start Spark & prepare DataFrames (snake_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a5c81dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark connected to Cassandra successfully.\n"
     ]
    }
   ],
   "source": [
    "# Ensure Python exe for executors\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Elhub-Part4\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\")\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(\"Spark connected to Cassandra successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9876d44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- startTime: timestamp (nullable = true)\n",
      " |-- endTime: timestamp (nullable = true)\n",
      " |-- lastUpdatedTime: timestamp (nullable = true)\n",
      " |-- priceArea: string (nullable = true)\n",
      " |-- productionGroup: string (nullable = true)\n",
      " |-- quantityKwh: double (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+-------------------+---------+---------------+-----------+\n",
      "|startTime          |endTime            |lastUpdatedTime    |priceArea|productionGroup|quantityKwh|\n",
      "+-------------------+-------------------+-------------------+---------+---------------+-----------+\n",
      "|2022-01-01 00:00:00|2022-01-01 01:00:00|2025-02-01 18:02:57|NO1      |hydro          |1291422.4  |\n",
      "|2022-01-01 01:00:00|2022-01-01 02:00:00|2025-02-01 18:02:57|NO1      |hydro          |1246209.4  |\n",
      "|2022-01-01 02:00:00|2022-01-01 03:00:00|2025-02-01 18:02:57|NO1      |hydro          |1271757.0  |\n",
      "|2022-01-01 03:00:00|2022-01-01 04:00:00|2025-02-01 18:02:57|NO1      |hydro          |1204251.8  |\n",
      "|2022-01-01 04:00:00|2022-01-01 05:00:00|2025-02-01 18:02:57|NO1      |hydro          |1202086.9  |\n",
      "+-------------------+-------------------+-------------------+---------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df_prod_raw = spark.createDataFrame(df_prod_22_24)\n",
    "spark_df_prod_raw.printSchema()\n",
    "spark_df_prod_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a3c180f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- startTime: timestamp (nullable = true)\n",
      " |-- endTime: timestamp (nullable = true)\n",
      " |-- lastUpdatedTime: timestamp (nullable = true)\n",
      " |-- priceArea: string (nullable = true)\n",
      " |-- quantityKwh: double (nullable = true)\n",
      " |-- consumptionGroup: string (nullable = true)\n",
      " |-- meteringPointCount: long (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+-------------------+---------+-----------+----------------+------------------+\n",
      "|startTime          |endTime            |lastUpdatedTime    |priceArea|quantityKwh|consumptionGroup|meteringPointCount|\n",
      "+-------------------+-------------------+-------------------+---------+-----------+----------------+------------------+\n",
      "|2021-01-01 00:00:00|2021-01-01 01:00:00|2024-12-20 10:35:40|NO1      |177071.56  |cabin           |100607            |\n",
      "|2021-01-01 01:00:00|2021-01-01 02:00:00|2024-12-20 10:35:40|NO1      |171335.12  |cabin           |100607            |\n",
      "|2021-01-01 02:00:00|2021-01-01 03:00:00|2024-12-20 10:35:40|NO1      |164912.02  |cabin           |100607            |\n",
      "|2021-01-01 03:00:00|2021-01-01 04:00:00|2024-12-20 10:35:40|NO1      |160265.77  |cabin           |100607            |\n",
      "|2021-01-01 04:00:00|2021-01-01 05:00:00|2024-12-20 10:35:40|NO1      |159828.69  |cabin           |100607            |\n",
      "+-------------------+-------------------+-------------------+---------+-----------+----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark_df_cons_raw = spark.createDataFrame(df_cons_21_24)\n",
    "spark_df_cons_raw.printSchema()\n",
    "spark_df_cons_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d324cf0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2186035f",
   "metadata": {},
   "source": [
    "### 4) Merging the columns into a single `energy_group` field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87ea5fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- end_time: timestamp (nullable = true)\n",
      " |-- last_updated_time: timestamp (nullable = true)\n",
      " |-- price_area: string (nullable = true)\n",
      " |-- energy_group: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- end_time: timestamp (nullable = true)\n",
      " |-- last_updated_time: timestamp (nullable = true)\n",
      " |-- price_area: string (nullable = true)\n",
      " |-- energy_group: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def normalize_columns(sdf):\n",
    "    cols = set(sdf.columns)\n",
    "    if \"productionGroup\" in cols:\n",
    "        grp = F.col(\"productionGroup\")\n",
    "    elif \"consumptionGroup\" in cols:\n",
    "        grp = F.col(\"consumptionGroup\")\n",
    "    else:\n",
    "        raise ValueError(f\"No group column found. Columns: {sorted(cols)}\")\n",
    "    last_upd = F.col(\"lastUpdatedTime\") if \"lastUpdatedTime\" in cols else F.lit(None).cast(\"timestamp\")\n",
    "    return sdf.select(\n",
    "        F.col(\"startTime\").alias(\"start_time\"),\n",
    "        F.col(\"endTime\").alias(\"end_time\"),\n",
    "        last_upd.alias(\"last_updated_time\"),\n",
    "        F.col(\"priceArea\").alias(\"price_area\"),\n",
    "        grp.alias(\"energy_group\"),\n",
    "        F.col(\"quantityKwh\").alias(\"value\")\n",
    "    )\n",
    "\n",
    "sdf_prod_22_24 = normalize_columns(spark_df_prod_raw)\n",
    "sdf_cons_21_24 = normalize_columns(spark_df_cons_raw)\n",
    "\n",
    "sdf_prod_22_24.printSchema()\n",
    "sdf_cons_21_24.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f5ccccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_prod_22_24.write.format(\"org.apache.spark.sql.cassandra\").mode(\"append\")\n",
    "    .option(\"keyspace\",\"energy_data\").option(\"table\",\"production_2022_2024\").save()\n",
    ")\n",
    "(\n",
    "    sdf_cons_21_24.write.format(\"org.apache.spark.sql.cassandra\").mode(\"append\")\n",
    "    .option(\"keyspace\",\"energy_data\").option(\"table\",\"consumption_2021_2024\").save()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d53ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prod rows: 657525 cons rows: 876500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Quick Check\n",
    "prod_chk = spark.read.format(\"org.apache.spark.sql.cassandra\").options(\n",
    "    keyspace=\"energy_data\", table=\"production_2022_2024\").load()\n",
    "cons_chk = spark.read.format(\"org.apache.spark.sql.cassandra\").options(\n",
    "    keyspace=\"energy_data\", table=\"consumption_2021_2024\").load()\n",
    "print(\"prod rows:\", prod_chk.count(), \"cons rows:\", cons_chk.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc3d63b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from pymongo.errors import ConnectionFailure, ConfigurationError\n",
    "import os, pandas as pd\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "user = os.getenv(\"MONGO_USER\")\n",
    "password = quote_plus(os.getenv(\"MONGO_PASS\") or \"\")\n",
    "cluster = os.getenv(\"MONGO_CLUSTER\")\n",
    "\n",
    "if not all([user, password, cluster]):\n",
    "    raise SystemExit(\"Missing MongoDB credentials in .env\")\n",
    "\n",
    "uri = f\"mongodb+srv://{user}:{password}@{cluster}/?retryWrites=true&w=majority\"\n",
    "\n",
    "try:\n",
    "    client = MongoClient(uri)\n",
    "    client.admin.command(\"ping\")\n",
    "    print(\"Successfully connected to MongoDB!\")\n",
    "except (ConnectionFailure, ConfigurationError) as e:\n",
    "    raise SystemExit(f\"MongoDB connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e466768c",
   "metadata": {},
   "source": [
    "## Helper to write any Spark DF → Mongo (idempotent upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "085ce775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_to_mongo_upsert(sdf, db_name: str, coll_name: str):\n",
    "    \"\"\"\n",
    "    Writes a Spark DF with columns [price_area, energy_group, start_time, value]\n",
    "    into MongoDB using an idempotent bulk upsert and a unique compound index.\n",
    "    \"\"\"\n",
    "    db = client[db_name]\n",
    "    coll = db[coll_name]\n",
    "\n",
    "    # Unique index to prevent duplicates\n",
    "    coll.create_index(\n",
    "        [(\"price_area\", 1), (\"energy_group\", 1), (\"start_time\", 1)],\n",
    "        unique=True,\n",
    "        name=\"uniq_price_group_time\"\n",
    "    )\n",
    "\n",
    "    # Convert Spark → pandas for bulk_write\n",
    "    pdf = sdf.select(\"price_area\", \"energy_group\", \"start_time\", \"value\").toPandas()\n",
    "    pdf[\"start_time\"] = pd.to_datetime(pdf[\"start_time\"], errors=\"coerce\")\n",
    "\n",
    "    records = pdf.to_dict(\"records\")\n",
    "    if not records:\n",
    "        print(f\"No records to write for {db_name}.{coll_name}\")\n",
    "        return\n",
    "\n",
    "    ops = [\n",
    "        UpdateOne(\n",
    "            {\"price_area\": r[\"price_area\"], \"energy_group\": r[\"energy_group\"], \"start_time\": r[\"start_time\"]},\n",
    "            {\"$set\": r},\n",
    "            upsert=True,\n",
    "        )\n",
    "        for r in records\n",
    "    ]\n",
    "\n",
    "    result = coll.bulk_write(ops, ordered=False)\n",
    "    print(f\"[{db_name}.{coll_name}] MongoDB upsert done\")\n",
    "    print(\"  Matched:\", result.matched_count)\n",
    "    print(\"  Modified:\", result.modified_count)\n",
    "    print(\"  Upserted:\", len(result.upserted_ids))\n",
    "    print(\"  Total attempted:\", len(ops))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22daabe",
   "metadata": {},
   "source": [
    "## Write the new Part 4 datasets (production & consumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cc28119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[elhub_data.production_2022_2024] MongoDB upsert done\n",
      "  Matched: 200050\n",
      "  Modified: 87\n",
      "  Upserted: 457550\n",
      "  Total attempted: 657600\n",
      "[elhub_data.consumption_2021_2024] MongoDB upsert done\n",
      "  Matched: 100\n",
      "  Modified: 100\n",
      "  Upserted: 876500\n",
      "  Total attempted: 876600\n"
     ]
    }
   ],
   "source": [
    "# Database/Collections for Part 4 (separate from your Part 2 collection)\n",
    "DB_NAME = \"elhub_data\"\n",
    "COLL_PROD = \"production_2022_2024\"\n",
    "COLL_CONS = \"consumption_2021_2024\"\n",
    "\n",
    "spark_to_mongo_upsert(sdf_prod_22_24, DB_NAME, COLL_PROD)\n",
    "spark_to_mongo_upsert(sdf_cons_21_24, DB_NAME, COLL_CONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f48cec5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production docs: 657525\n",
      "Consumption docs: 876500\n"
     ]
    }
   ],
   "source": [
    "prod_coll = client[DB_NAME][COLL_PROD]\n",
    "cons_coll = client[DB_NAME][COLL_CONS]\n",
    "\n",
    "print(\"Production docs:\", prod_coll.count_documents({}))\n",
    "print(\"Consumption docs:\", cons_coll.count_documents({}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6fa0e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production areas: ['NO1', 'NO2', 'NO3', 'NO4', 'NO5']\n",
      "Consumption areas: ['NO1', 'NO2', 'NO3', 'NO4', 'NO5']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Production areas:\", sorted(prod_coll.distinct(\"price_area\")))\n",
    "print(\"Consumption areas:\", sorted(cons_coll.distinct(\"price_area\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b43d0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production groups: ['hydro', 'other', 'solar', 'thermal', 'wind']\n",
      "Consumption groups: ['cabin', 'household', 'primary', 'secondary', 'tertiary']\n",
      "Sample production doc (no _id): {'energy_group': 'hydro', 'price_area': 'NO1', 'start_time': datetime.datetime(2022, 1, 1, 0, 0), 'value': 1291422.4}\n",
      "Sample consumption doc (no _id): {'energy_group': 'cabin', 'price_area': 'NO1', 'start_time': datetime.datetime(2021, 1, 1, 0, 0), 'value': 177071.56}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Production groups:\", sorted(set(prod_coll.distinct(\"energy_group\"))))\n",
    "print(\"Consumption groups:\", sorted(set(cons_coll.distinct(\"energy_group\"))))\n",
    "\n",
    "# Peek a doc\n",
    "print(\"Sample production doc (no _id):\", prod_coll.find_one({}, {\"_id\": 0}))\n",
    "print(\"Sample consumption doc (no _id):\", cons_coll.find_one({}, {\"_id\": 0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6339627",
   "metadata": {},
   "source": [
    "# Project Log – Part 4 (Machine Learning)\n",
    "\n",
    "In this part of the project, I expanded the ETL pipeline for Elhub data and added new analysis and forecasting features to the Streamlit app. The production data for 2021 had already been collected earlier in part 2, so in part 4 I focused on downloading and processing production data for 2022–2024 and consumption data for 2021–2024. After fetching the data from the Elhub API, I cleaned the fields, unified the time format, and uploaded the results to both Cassandra and MongoDB using upsert operations so the new years were added without creating duplicates.\n",
    "\n",
    "In the Streamlit application, I improved the map page by using Folium and GeoJSON to display the five Norwegian price areas (NO1–NO5). The map colors the areas based on the average production or consumption selected by the user. I faced several issues related to map boundaries and layer performance, especially when displaying municipalities, but after several attempts and using AI assistance as guidance, I managed to stabilize the map by adding `max_bounds`, `min_zoom`, and by loading municipality layers only when needed.\n",
    "\n",
    "The Snow Drift page uses the Tabler (2003) model together with hourly weather data from the ERA5 API. The calculations depend on the clicked coordinate from the map. I implemented yearly Qt values, a wind rose, and fence height estimates, and also added an option for monthly snow-drift results. All results are displayed with interactive Plotly charts.\n",
    "\n",
    "In the Meteo Correlation page, I converted the sliding-window correlation method into an interactive tool. The user can choose a weather variable, a price area, and an energy group. From my tests, temperature showed a clear negative relationship with energy during colder periods, wind had a weak and unstable correlation, and precipitation showed a small positive effect but not a strong or consistent one.\n",
    "\n",
    "For forecasting, I built a complete SARIMAX interface where the user can select model parameters, training range, forecast horizon, and optional weather variables. The model captured the seasonal patterns fairly well, and the evaluation values (AIC/BIC) were reasonable.\n",
    "\n",
    "During the work, I faced several technical challenges such as slow weather API queries, map rendering issues, and some MongoDB connection timeouts. Most of these were solved by using caching, reorganizing parts of the code, and adjusting connection settings. Overall, this part of the project helped me connect ETL work with interactive dashboards and time-series analysis.\n",
    "\n",
    "---\n",
    "\n",
    "# AI Usage\n",
    "\n",
    "During this project, I used AI tools in a limited way as technical help, not as a replacement for my work. I used ChatGPT mainly to understand some errors I faced in Streamlit, especially problems related to the map, GeoJSON layers, and session state. I also used it to understand a few SARIMAX warnings and to explore possible solutions.\n",
    "\n",
    "AI was used as a **guide** to point me in the right direction or suggest improvements, but all the main coding — ETL, MongoDB queries, Streamlit pages, correlation logic, and forecasting — was written, tested, and adjusted by me.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bbeb61",
   "metadata": {},
   "source": [
    "### Map and Selectors — Overview\n",
    "\n",
    "This screenshot shows the interactive GeoJSON map with Norwegian price areas (NO1–NO5).  \n",
    "Users can filter dataset type, price areas, energy groups, and date range.  \n",
    "The map colours each area based on mean production/consumption for the selected period.\n",
    "\n",
    "Clicked coordinates are stored and later used in the Snow Drift and SARIMAX pages.  \n",
    "The table on the right confirms correct aggregation for each price area.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb6f731b",
   "metadata": {},
   "source": [
    "![Skjermbilde 2025-11-27 173606.png](<Skjermbilde 2025-11-27 173606.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e070e07d",
   "metadata": {},
   "source": [
    "### Snow Drift Page  (Tabler 2003)\n",
    "\n",
    "The Snow Drift page calculates snow drift using the **Tabler (2003)** method and ERA5 weather data for the selected map location.  \n",
    "The yearly results (July–June) show how the snow drift (Qt) changes from one season to another, with a line that also shows the overall average.  \n",
    "The monthly Qt values help identify the periods with the most snow movement, usually during the winter months.  \n",
    "The **wind rose** plot shows the main wind directions that push the snow, with the strongest transport coming mostly from the west and some south-eastern directions.  \n",
    "The fence height table uses the Qt values to estimate how high different fence types need to be each season, giving a simple way to understand the practical impact of the snow drift.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4946ad3d",
   "metadata": {},
   "source": [
    "![Skjermbilde 2025-11-27 175050.png](<Skjermbilde 2025-11-27 175050.png>) ![Skjermbilde 2025-11-27 175130.png](<Skjermbilde 2025-11-27 175130.png>) ![Skjermbilde 2025-11-27 175117.png](<Skjermbilde 2025-11-27 175117.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e9b9c0",
   "metadata": {},
   "source": [
    "### Meteo–Energy Correlation \n",
    "\n",
    "The Meteo–Energy Correlation page shows how weather variables and energy production or consumption change together over time. The page uses a sliding-window correlation, where the user can choose the window size, lag, price area, energy group, and weather variables. The time-series plots display the selected energy data together with the weather data (such as temperature, wind speed, or precipitation), and the green line shows the correlation for each moment in time. These visuals help reveal short-term and long-term relationships, for example small positive correlation with temperature, weak and noisy patterns with wind speed, and sometimes a clearer connection between precipitation and hydro production. A small table also shows the most recent correlation value for the selected settings.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3018e023",
   "metadata": {},
   "source": [
    "![image-3.png](image-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e5c79b",
   "metadata": {},
   "source": [
    "### Forecast SARIMAX Page\n",
    "\n",
    "This screenshot shows the SARIMAX forecasting page, where the user selects the dataset type, price area, energy group, and training period.  \n",
    "Once the model is trained, the app displays AIC/BIC values and plots both the historical data and the future forecast with a 95% confidence interval.  \n",
    "The green message confirms that the forecast was completed successfully.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "429a04fe",
   "metadata": {},
   "source": [
    "![Skjermbilde 2025-11-27 184841.png](<Skjermbilde 2025-11-27 184841.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6f1dfb",
   "metadata": {},
   "source": [
    "### Data Overview \n",
    "\n",
    "The Data Overview page gives a simple way to explore the energy data stored in MongoDB. The user can choose the dataset type, price area, energy group, date range, and number of rows to load. The page then shows basic summary statistics such as the number of rows, the date span, and the maximum, minimum, mean, and standard deviation of the selected data. A time-series plot displays how the energy values change over time, making it easy to see trends or daily patterns. A full data table is also provided so the user can inspect the raw values directly, and the data can be downloaded as CSV or JSON for further analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77d564b2",
   "metadata": {},
   "source": [
    "![Skjermbilde 2025-11-27 180133.png](<Skjermbilde 2025-11-27 180133.png>) ![Skjermbilde 2025-11-27 180236.png](<Skjermbilde 2025-11-27 180236.png>)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ind320",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
