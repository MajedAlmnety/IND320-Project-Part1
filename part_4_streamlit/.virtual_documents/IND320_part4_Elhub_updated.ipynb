


# !pip install python-dotenv pymongo dnspython pyspark plotly requests
# Spark Cassandra connector is provided via spark.jars.packages in the SparkSession builder.

from datetime import datetime, timedelta
from typing import List, Dict, Tuple
import time, os, json
import requests
import pandas as pd
from zoneinfo import ZoneInfo
from dotenv import load_dotenv, find_dotenv

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *









# Elhub API
ELHUB_BASE_URL = "https://api.elhub.no/energy-data/v0/price-areas"
DATASET_PROD = "PRODUCTION_PER_GROUP_MBA_HOUR"
DATASET_CONS = "CONSUMPTION_PER_GROUP_MBA_HOUR"

UTC = ZoneInfo("UTC")
PAUSE_S = 0.4
TIMEOUT_S = 30

# Ranges
PROD_YEARS = [2022, 2023, 2024]        # append after your 2021 data (already done in Part 2)
CONS_YEARS = [2021, 2022, 2023, 2024]  # new dataset

# Utility: build monthly (start, end) UTC windows for a given year
def monthly_windows(year: int) -> List[Tuple[datetime, datetime]]:
    wins = []
    for m in range(1, 13):
        start = datetime(year, m, 1, 0, 0, 0, tzinfo=UTC)
        next_year = year + (m // 12)
        next_month = (m % 12) + 1
        end = datetime(next_year, next_month, 1, 0, 0, 0, tzinfo=UTC)
        wins.append((start, end))
    return wins






session = requests.Session()

def fetch_elhub(dataset: str, start_dt: datetime, end_dt: datetime) -> List[Dict]:
    """Fetch one monthly window from Elhub and return flattened hourly records list."""
    params = {
        "dataset": dataset,
        "startDate": start_dt.isoformat(),  # UTC (no DST issues)
        "endDate": end_dt.isoformat(),
    }
    try:
        r = session.get(ELHUB_BASE_URL, params=params, timeout=TIMEOUT_S)
        r.raise_for_status()
        payload = r.json()
    except requests.RequestException as e:
        print(f"{start_dt:%Y-%m} — omitted (I/O): {e}")
        return []

    out = []
    for area in payload.get("data", []):
        attrs = area.get("attributes") or {}
        # key is productionPerGroupMbaHour OR consumptionPerGroupMbaHour depending on dataset
        key = "productionPerGroupMbaHour" if "PRODUCTION" in dataset else "consumptionPerGroupMbaHour"
        chunk = attrs.get(key) or []
        if isinstance(chunk, list):
            out.extend(chunk)
    return out

def fetch_years(dataset: str, years: List[int]) -> pd.DataFrame:
    all_records = []
    print(f"Fetching {dataset} for years: {years}")
    for y in years:
        for (start_dt, end_dt) in monthly_windows(y):
            rows = fetch_elhub(dataset, start_dt, end_dt)
            all_records.extend(rows)
            time.sleep(PAUSE_S)
    if not all_records:
        print("No data retrieved.")
        return pd.DataFrame()
    df = pd.DataFrame(all_records)
    # Normalize times to UTC (keep tz-aware)
    for col in ("startTime", "endTime", "lastUpdatedTime"):
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], utc=True, errors="coerce")
    # Order columns
    preferred = ["startTime", "endTime", "lastUpdatedTime", "priceArea", "productionGroup", "quantityKwh"]
    ordered = [c for c in preferred if c in df.columns] + [c for c in df.columns if c not in preferred]
    df = df[ordered]
    print(f"Retrieved rows: {len(df):,}")
    return df






df_prod_22_24 = fetch_years(DATASET_PROD, PROD_YEARS)
df_cons_21_24 = fetch_years(DATASET_CONS, CONS_YEARS)

display(df_prod_22_24.head())
display(df_prod_22_24.shape)
display(df_cons_21_24.head())
display(df_cons_21_24.shape)









# Ensure Python exe for executors
import sys
os.environ["PYSPARK_PYTHON"] = sys.executable
os.environ["PYSPARK_DRIVER_PYTHON"] = sys.executable

spark = (
    SparkSession.builder
    .appName("Elhub-Part4")
    .config("spark.cassandra.connection.host", "127.0.0.1")
    .config("spark.cassandra.connection.port", "9042")
    .config("spark.jars.packages", "com.datastax.spark:spark-cassandra-connector_2.12:3.5.1")
    .getOrCreate()
)
print("Spark connected to Cassandra successfully.")


spark_df_prod_raw = spark.createDataFrame(df_prod_22_24)
spark_df_prod_raw.printSchema()
spark_df_prod_raw.show(5, truncate=False)



spark_df_cons_raw = spark.createDataFrame(df_cons_21_24)
spark_df_cons_raw.printSchema()
spark_df_cons_raw.show(5, truncate=False)








def normalize_columns(sdf):
    cols = set(sdf.columns)
    if "productionGroup" in cols:
        grp = F.col("productionGroup")
    elif "consumptionGroup" in cols:
        grp = F.col("consumptionGroup")
    else:
        raise ValueError(f"No group column found. Columns: {sorted(cols)}")
    last_upd = F.col("lastUpdatedTime") if "lastUpdatedTime" in cols else F.lit(None).cast("timestamp")
    return sdf.select(
        F.col("startTime").alias("start_time"),
        F.col("endTime").alias("end_time"),
        last_upd.alias("last_updated_time"),
        F.col("priceArea").alias("price_area"),
        grp.alias("energy_group"),
        F.col("quantityKwh").alias("value")
    )

sdf_prod_22_24 = normalize_columns(spark_df_prod_raw)
sdf_cons_21_24 = normalize_columns(spark_df_cons_raw)

sdf_prod_22_24.printSchema()
sdf_cons_21_24.printSchema()



(
    sdf_prod_22_24.write.format("org.apache.spark.sql.cassandra").mode("append")
    .option("keyspace","energy_data").option("table","production_2022_2024").save()
)
(
    sdf_cons_21_24.write.format("org.apache.spark.sql.cassandra").mode("append")
    .option("keyspace","energy_data").option("table","consumption_2021_2024").save()
)




### Quick Check
prod_chk = spark.read.format("org.apache.spark.sql.cassandra").options(
    keyspace="energy_data", table="production_2022_2024").load()
cons_chk = spark.read.format("org.apache.spark.sql.cassandra").options(
    keyspace="energy_data", table="consumption_2021_2024").load()
print("prod rows:", prod_chk.count(), "cons rows:", cons_chk.count())



from dotenv import load_dotenv, find_dotenv
from urllib.parse import quote_plus
from pymongo import MongoClient, UpdateOne
from pymongo.errors import ConnectionFailure, ConfigurationError
import os, pandas as pd

load_dotenv(find_dotenv())

user = os.getenv("MONGO_USER")
password = quote_plus(os.getenv("MONGO_PASS") or "")
cluster = os.getenv("MONGO_CLUSTER")

if not all([user, password, cluster]):
    raise SystemExit("Missing MongoDB credentials in .env")

uri = f"mongodb+srv://{user}:{password}@{cluster}/?retryWrites=true&w=majority"

try:
    client = MongoClient(uri)
    client.admin.command("ping")
    print("Successfully connected to MongoDB!")
except (ConnectionFailure, ConfigurationError) as e:
    raise SystemExit(f"MongoDB connection failed: {e}")





def spark_to_mongo_upsert(sdf, db_name: str, coll_name: str):
    """
    Writes a Spark DF with columns [price_area, energy_group, start_time, value]
    into MongoDB using an idempotent bulk upsert and a unique compound index.
    """
    db = client[db_name]
    coll = db[coll_name]

    # Unique index to prevent duplicates
    coll.create_index(
        [("price_area", 1), ("energy_group", 1), ("start_time", 1)],
        unique=True,
        name="uniq_price_group_time"
    )

    # Convert Spark → pandas for bulk_write
    pdf = sdf.select("price_area", "energy_group", "start_time", "value").toPandas()
    pdf["start_time"] = pd.to_datetime(pdf["start_time"], errors="coerce")

    records = pdf.to_dict("records")
    if not records:
        print(f"No records to write for {db_name}.{coll_name}")
        return

    ops = [
        UpdateOne(
            {"price_area": r["price_area"], "energy_group": r["energy_group"], "start_time": r["start_time"]},
            {"$set": r},
            upsert=True,
        )
        for r in records
    ]

    result = coll.bulk_write(ops, ordered=False)
    print(f"[{db_name}.{coll_name}] MongoDB upsert done")
    print("  Matched:", result.matched_count)
    print("  Modified:", result.modified_count)
    print("  Upserted:", len(result.upserted_ids))
    print("  Total attempted:", len(ops))






# Database/Collections for Part 4 (separate from your Part 2 collection)
DB_NAME = "elhub_data"
COLL_PROD = "production_2022_2024"
COLL_CONS = "consumption_2021_2024"

spark_to_mongo_upsert(sdf_prod_22_24, DB_NAME, COLL_PROD)
spark_to_mongo_upsert(sdf_cons_21_24, DB_NAME, COLL_CONS)



prod_coll = client[DB_NAME][COLL_PROD]
cons_coll = client[DB_NAME][COLL_CONS]

print("Production docs:", prod_coll.count_documents({}))
print("Consumption docs:", cons_coll.count_documents({}))





print("Production areas:", sorted(prod_coll.distinct("price_area")))
print("Consumption areas:", sorted(cons_coll.distinct("price_area")))




print("Production groups:", sorted(set(prod_coll.distinct("energy_group"))))
print("Consumption groups:", sorted(set(cons_coll.distinct("energy_group"))))

# Peek a doc
print("Sample production doc (no _id):", prod_coll.find_one({}, {"_id": 0}))
print("Sample consumption doc (no _id):", cons_coll.find_one({}, {"_id": 0}))




































