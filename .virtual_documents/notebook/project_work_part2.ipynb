




# Core setup 
from datetime import datetime
import requests
import pandas as pd
from zoneinfo import ZoneInfo 
import os
import time
import json
from datetime import datetime, timedelta
from typing import List, Dict
from zoneinfo import ZoneInfo
import requests
import pandas as pd
import plotly.express as px
from dotenv import load_dotenv
from pymongo import MongoClient
from pyspark.sql import SparkSession
from pyspark.sql.functions import sum as spark_sum, month
# Configuration
ELHUB_BASE_URL = "https://api.elhub.no/energy-data/v0/price-areas"
DATASET = "PRODUCTION_PER_GROUP_MBA_HOUR"
#OSLO = ZoneInfo("Europe/Oslo")
UTC = ZoneInfo("UTC")

YEAR = 2021        
PAUSE_S = 0.4      # gentle pause between API calls
TIMEOUT_S = 30     # request timeout



# Construct (start_of_month, start_of_next_month) for the year
month_windows = []
for m in range(1, 12 + 1):  # Loop through elements
    start = datetime(YEAR, m, 1, 0, 0, 0, tzinfo=UTC)
    next_year = YEAR + (m // 12)
    next_month = (m % 12) + 1
    end = datetime(next_year, next_month, 1, 0, 0, 0, tzinfo=UTC)
    month_windows.append((start, end))

# quick peek of the first three windows (non-verbose)
month_windows[:3]






all_records = []

session = requests.Session()
print("Fetching 2021 Elhub production records, please wait...")
for start_dt, end_dt in month_windows:  # Loop through elements
    start_iso = start_dt.isoformat()  # honors DST for Europe/Oslo
    end_iso   = end_dt.isoformat()
    params = {"dataset": DATASET, "startDate": start_iso, "endDate": end_iso}

    try:
        r = session.get(ELHUB_BASE_URL, params=params, timeout=TIMEOUT_S)
        r.raise_for_status()
        payload = r.json()
    except requests.RequestException as e:
        # understated academic note; proceed
        print(f"{start_dt:%Y-%m} — omitted (transient I/O): {e}")
        time.sleep(PAUSE_S)
        continue

    # count for this month only
    n_month = 0

    # extract attributes.productionPerGroupMbaHour, if present
    for area in payload.get("data", []):
        attrs = area.get("attributes") or {}
        chunk = attrs.get("productionPerGroupMbaHour") or []
        if isinstance(chunk, list):
            all_records.extend(chunk)
            n_month += len(chunk)

    time.sleep(PAUSE_S)

# avoid auto-displaying the grand total in notebooks
grand_total = len(all_records)

if grand_total == 0:
    print("No data retrieved.")
else:
    print(f"\nData retrieved successfully — total records: {grand_total}")




# To pandas; normalize time columns to Europe/Oslo
df = pd.DataFrame(all_records)

# Normalize temporal columns (if present) to Europe/Oslo
for col in ("startTime", "endTime", "lastUpdatedTime"):  # Loop through elements
    if col in df.columns:
        df[col] = pd.to_datetime(df[col], utc=True, errors="coerce").dt.tz_convert(UTC)

# Optional tidy ordering
preferred = ["startTime", "endTime", "lastUpdatedTime", "priceArea", "productionGroup", "quantityKwh"]
ordered = [c for c in preferred if c in df.columns] + [c for c in df.columns if c not in preferred]
df = df[ordered]

# compact sanity check (shape + head in one line-like view)
df.shape


df.head()







import sys, os  # Import required libraries

# Python interpreter for driver and workers
os.environ["PYSPARK_PYTHON"] = sys.executable
os.environ["PYSPARK_DRIVER_PYTHON"] = sys.executable

spark = (
    SparkSession.builder
    .appName("Elhub-Cassandra")
    .config("spark.cassandra.connection.host", "127.0.0.1")
    .config("spark.cassandra.connection.port", "9042")
    .config("spark.jars.packages", "com.datastax.spark:spark-cassandra-connector_2.12:3.5.1")
    .getOrCreate()
)

print("Spark connected to Cassandra successfully.")




print("Converting pandas DataFrame to Spark DataFrame...")

# Convert
spark_df = spark.createDataFrame(df)
# Confirm schema and preview data
print("Conversion successful — Spark DataFrame schema:")
spark_df.printSchema()

print("\nFirst 5 rows:")
spark_df.show(5, truncate=False)



spark_df.describe().show()


spark_df.columns



from dotenv import load_dotenv, find_dotenv
import os
from urllib.parse import quote_plus

load_dotenv(find_dotenv())

user = os.getenv("MONGO_USER")
password = quote_plus(os.getenv("MONGO_PASS") or "")
cluster = os.getenv("MONGO_CLUSTER")

if not all([user, password, cluster]):  # Conditional check
    raise SystemExit("Missing MongoDB credentials in .env")

# Don’t print full URI (avoid leaking secrets). Printing the cluster is fine.
print(f"Connecting to MongoDB cluster: {cluster}")

uri = f"mongodb+srv://{user}:{password}@{cluster}/?retryWrites=true&w=majority"

print(f"uri MongoDB: {uri}")














# create a new DataFrame with renamed columns
from pyspark.sql import functions as F  
spark_df = spark_df.select(
    F.col("startTime").alias("start_time"),         
    F.col("endTime").alias("end_time"),              
    F.col("lastUpdatedTime").alias("last_updated_time"),  
    F.col("priceArea").alias("price_area"),          
    F.col("productionGroup").alias("production_group"),  
    F.col("quantityKwh").alias("value")             
)



spark_df.columns


spark_df.describe().show()









(
    spark_df.write
    .format("org.apache.spark.sql.cassandra")  # Use Cassandra Spark connector
    .mode("append")                            # Append new rows (do not overwrite existing data)
    .option("keyspace", "energy_data")         # Cassandra keyspace name
    .option("table", "production_2021")        # Target table for production data  # Loop through elements
    .save()                                    # Trigger the write operation
)

print("Data successfully inserted into Cassandra (energy_data.production_2021)")







# Read the table from Cassandra
df = (spark.read
      .format("org.apache.spark.sql.cassandra")
      .options(keyspace="energy_data", table="production_2021")
      .load())

# Print the schema 
df.printSchema()

# 3️Shw the first 5 rows to 
df.select("price_area", "production_group", "start_time", "value").show(5, truncate=False)






# Read from Cassandra (narrow columns early)

from pyspark.sql import functions as F  
# Choose which price area and year to visualize
#price_area = "NO1"
year_label = "2021"

# Read data from Cassandra
df = (
    spark.read
    .format("org.apache.spark.sql.cassandra")
    .option("keyspace", "energy_data")          # Cassandra keyspace (database)
    .option("table", "production_2021")         # Table to read
    .load()
    .select("price_area", "production_group", "start_time", "value")  # Only needed columns
    
)

df.cache()  # Cache in memory for faster re-use



df.show(5)


df.filter(df["price_area"] == "NO1").show(2)
#df.filter(df["price_area"] == "NO2").show(2)
#df.filter(df["price_area"] == "NO3").show(2)
#df.filter(df["price_area"] == "NO4").show(2)
#df.filter(df["price_area"] == "NO5").show(2)








# Group and sum total production per production group
df_area_agg = (
    df.groupBy("production_group")              # Group by production group (e.g. Wind, Hydro)
      .agg(F.sum("value").alias("total_quantity"))  # Sum total production value
      .orderBy(F.desc("total_quantity"))            
)

# Convert Spark DataFrame to pandas for Plotly
df_area_agg = df_area_agg.toPandas()


df_area_agg


# Calculate share (%) and sort for nicer plotting
df_area_agg["share"] = 100 * df_area_agg["total_quantity"] / df_area_agg["total_quantity"].sum()
df_area_agg = df_area_agg.sort_values("total_quantity", ascending=False).reset_index(drop=True)

# Decide where to show labels (inside if large enough)
inside_thresh = 5.0  # percent threshold for label placement  # Loop through elements
textpos = ["inside" if s >= inside_thresh else "outside" for s in df_area_agg["share"]]

# Create the pastel pie chart
fig = px.pie(
    df_area_agg,
    values="total_quantity",
    names="production_group",
    title=f"Total Production in ({year_label})",
    color_discrete_sequence=px.colors.qualitative.Pastel  # Soft colors
)

# Adjust the look of the chart
fig.update_traces(
    textinfo="percent+label",                      
    textposition=textpos,                          # Label placement (inside/outside)
    pull=[0.04] * len(df_area_agg),                # Slightly pull out all slices
    hovertemplate="%{label}<br>%{percent:.1%} (%{value:,.0f})<extra></extra>", 
    sort=False,
    direction="clockwise",
    insidetextorientation="horizontal"
)

# Layout tweaks for better readability
fig.update_layout(
    width=900, height=520,
    title=dict(x=0.5, y=0.98, xanchor="center", yanchor="top"),   # Center title
    legend=dict(
        title=None,
        orientation="v",
        y=0.5, yanchor="middle",
        x=1.05, xanchor="left"   # Move legend to right side
    ),
    margin=dict(l=40, r=160, t=60, b=40)  # Extra space for legend
)

# Show the interactive pie chart
fig.show()






YEAR = 2021
price_area = "NO4"  # change to NO2/NO1/NO3/NO5 for testing

# Ensure numeric type and handle nulls
df = df.withColumn("value", F.col("value").cast("double")).na.fill({"value": 0.0})

# January only + chosen price area
df_jan = df.where(
    (F.year(F.col("start_time")) == YEAR) &
    (F.month(F.col("start_time")) == 1) &
    (F.col("price_area") == price_area)
)

# Aggregate hourly per production group
df_hourly = (
    df_jan.groupBy(
        "production_group",
        F.date_trunc("hour", F.col("start_time")).alias("ts_hour")
    )
    .agg(F.sum("value").alias("total_production"))
)

# Guard
if df_hourly.rdd.isEmpty():
    raise ValueError(f"No January data found for price area '{price_area}' in {YEAR}.")

# To pandas and plot
pdf = df_hourly.toPandas().sort_values(["ts_hour", "production_group"])
pdf["ts_hour"] = pd.to_datetime(pdf["ts_hour"])

fig = px.line(
    pdf,
    x="ts_hour",
    y="total_production",
    color="production_group",
    title=f"Hourly Production by Group — {price_area} — January {YEAR}",
    labels={"ts_hour": "Time", "total_production": "Production (MWh)", "production_group": "Group"}
)
fig.update_traces(mode="lines", hovertemplate="<b>%{fullData.name}</b><br>%{x}<br>%{y:,.0f} MWh<extra></extra>")
fig.update_layout(width=1100, height=520, legend_title_text="Production group", margin=dict(l=40, r=20, t=70, b=40))
fig.show()






# pip install python-dotenv pymongo dnspython
from dotenv import load_dotenv, find_dotenv  # Import required libraries
from urllib.parse import quote_plus
import os
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure, ConfigurationError


load_dotenv(find_dotenv())

user = os.getenv("MONGO_USER")
password = quote_plus(os.getenv("MONGO_PASS") or "")
cluster = os.getenv("MONGO_CLUSTER")

if not all([user, password, cluster]):  # Conditional check
    raise SystemExit("Missing MongoDB credentials in .env")

#print(f"Connecting to MongoDB cluster: {cluster}")

uri = f"mongodb+srv://{user}:{password}@{cluster}/?retryWrites=true&w=majority"




try:
    client = MongoClient(uri)
    # test connection
    client.admin.command('ping')
    print("Successfully connected to MongoDB!")
except (ConnectionFailure, ConfigurationError) as e:
    print("MongoDB connection failed:", e)



# Select only the important columns as required in the project
df_selected = df.select("price_area", "production_group", "start_time", "value")

# Convert Spark DataFrame to a pandas DataFrame
pdf = df_selected.toPandas()

# Display the first 5 rows to verify the data
pdf.head()



# --- Insert Spark-extracted data into MongoDB safely ---
from pymongo import MongoClient, UpdateOne  # Import required libraries
import pandas as pd

# Ensure 'start_time' column is in proper datetime format
pdf["start_time"] = pd.to_datetime(pdf["start_time"])

#  Connect to MongoDB
client = MongoClient(uri)  
db = client["elhub_data"]      
collection = db["energy_data_2021"]

# Create a unique index to prevent duplicates  (price_area + production_group + start_time)
collection.create_index(
    [("price_area", 1), ("production_group", 1), ("start_time", 1)],
    unique=True,
    name="uniq_price_group_time"
)

# Convert pandas DataFrame to a list of dictionaries (MongoDB documents)
records = pdf.to_dict("records")

# Prepare bulk upsert operations: 
# if a document exists -> update it, otherwise insert it
ops = [
    UpdateOne(
        {
            "price_area": r["price_area"],
            "production_group": r["production_group"],
            "start_time": r["start_time"],
        },
        {"$set": r},
        upsert=True
    )
    for r in records
]

# Execute the bulk operation
if ops:
    result = collection.bulk_write(ops, ordered=False)
    print("MongoDB upsert done")
    print("Matched:", result.matched_count)
    print("Modified:", result.modified_count)
    print("Upserted:", len(result.upserted_ids))
    print("Total attempted:", len(ops))
else:
    print("No records to insert.")






# Show one sample document (without the _id field)
collection.find_one({}, {"_id": 0})



# Check distinct values for price_area and production_group
areas = collection.distinct("price_area")
groups = collection.distinct("production_group")

print("Distinct price areas:", areas)
print("Distinct production groups:", groups)


collection.count_documents({})




